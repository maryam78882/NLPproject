{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":915247,"sourceType":"datasetVersion","datasetId":492069}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:30.896783Z","iopub.execute_input":"2025-05-12T19:56:30.897042Z","iopub.status.idle":"2025-05-12T19:56:31.168610Z","shell.execute_reply.started":"2025-05-12T19:56:30.897019Z","shell.execute_reply":"2025-05-12T19:56:31.167778Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arabic-to-english-translation-sentences/ara_eng.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets sentencepiece scikit-learn\n!!pip install bert-score --no-deps\n!pip install nltk\n!pip install gradio --quiet \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:33.448733Z","iopub.execute_input":"2025-05-12T19:56:33.449092Z","iopub.status.idle":"2025-05-12T19:56:53.356596Z","shell.execute_reply.started":"2025-05-12T19:56:33.449069Z","shell.execute_reply":"2025-05-12T19:56:53.355894Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom transformers import (\n    MarianTokenizer, MarianMTModel, MarianConfig,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback, GenerationConfig\n)\n# from nltk.translate.bleu_score import corpus_bleu,SmoothingFunction\nfrom bert_score import score as bert_score\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:56:56.491027Z","iopub.execute_input":"2025-05-12T19:56:56.491626Z","iopub.status.idle":"2025-05-12T19:57:39.085304Z","shell.execute_reply.started":"2025-05-12T19:56:56.491590Z","shell.execute_reply":"2025-05-12T19:57:39.084756Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 19:57:19.923658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747079840.361027      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747079840.493434      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nimport string\nimport unicodedata\n\n# Load the dataset\npath = \"/kaggle/input/arabic-to-english-translation-sentences/ara_eng.txt\"\n\ndf = pd.read_csv(path, sep='\\t', names=[\"english\", \"arabic\"], encoding=\"utf-8\")\n\n# Define English text cleaning\ndef clean_english(text):\n    text = text.lower()\n    text = unicodedata.normalize(\"NFKD\", text)  # Normalize accented characters\n    text = re.sub(r'\\([^)]*\\)', '', text)  # Remove things in parentheses\n    text = re.sub(r'\\[.*?\\]', '', text)  # Remove things in brackets\n    text = re.sub(r\"[^a-z0-9.,!?']+\", \" \", text)  # Keep punctuation that might help context\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Define Arabic text cleaning\ndef clean_arabic(text):\n    text = re.sub(r'[^\\u0600-\\u06FF\\s]', ' ', text)  # Remove non-Arabic characters\n    text = re.sub(r'[ًٌٍَُِّْـ]', '', text)  # Remove short vowels (tashkeel)\n    text = re.sub(r'[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06ED]', '', text)  # Additional diacritics\n    text = re.sub(r'[\\d\\W_]', ' ', text)  # Remove digits and symbols\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Apply cleaning\ndf[\"english\"] = df[\"english\"].apply(clean_english)\ndf[\"arabic\"] = df[\"arabic\"].apply(clean_arabic)\n\n# Filter short or unhelpful sentences\ndf = df[(df[\"english\"].str.split().str.len() >= 3) & (df[\"arabic\"].str.split().str.len() >= 3)]\ndf = df[(df[\"english\"].str.len() > 5) & (df[\"arabic\"].str.len() > 5)]\n\n# Remove duplicates\ndf = df.drop_duplicates(subset=[\"english\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:43.242993Z","iopub.execute_input":"2025-05-12T19:57:43.243895Z","iopub.status.idle":"2025-05-12T19:57:44.824186Z","shell.execute_reply.started":"2025-05-12T19:57:43.243867Z","shell.execute_reply":"2025-05-12T19:57:44.823330Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:48.666466Z","iopub.execute_input":"2025-05-12T19:57:48.666749Z","iopub.status.idle":"2025-05-12T19:57:48.763519Z","shell.execute_reply.started":"2025-05-12T19:57:48.666727Z","shell.execute_reply":"2025-05-12T19:57:48.762614Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model_name = \"Helsinki-NLP/opus-mt-en-ar\"\n\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:57:51.787033Z","iopub.execute_input":"2025-05-12T19:57:51.787575Z","iopub.status.idle":"2025-05-12T19:57:55.646962Z","shell.execute_reply.started":"2025-05-12T19:57:51.787548Z","shell.execute_reply":"2025-05-12T19:57:55.646158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99a6773396bf4ce5aa3109ef0f9b5e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394636ff10334925bb17914bbed4c4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/917k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01d85b57fee428fb5bbbca1ed5a20a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d794ef99b63241658e01f09e3d670bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45d7a5db41d4753a3696514e3dbb3de"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1803a960944e8b98380130143a7a53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601860ebde2c4d84a325d73ca7e3283f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d626c779aa2a416ba66e26bf7e8dd464"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Preprocessing function for tokenizing input/target text\nmax_length = 128\n\ndef preprocess(batch):\n    inputs = tokenizer(batch[\"english\"], max_length=max_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(batch[\"arabic\"], max_length=max_length, truncation=True, padding=\"max_length\")\n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"english\", \"arabic\"])\ntest_dataset = test_dataset.map(preprocess, batched=True, remove_columns=[\"english\", \"arabic\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:58:05.048740Z","iopub.execute_input":"2025-05-12T19:58:05.049033Z","iopub.status.idle":"2025-05-12T19:58:13.239021Z","shell.execute_reply.started":"2025-05-12T19:58:05.049010Z","shell.execute_reply":"2025-05-12T19:58:13.238212Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16875 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6097978c95f1499e996e289a0d3a05be"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4219 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"400e2004d2884287be1e8e57074a8bc7"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # إزالة الـ -100 المستخدم في padding\n    labels = [[token for token in label if token != -100] for label in labels]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # حساب BLEU\n    # references_bleu = [[label.split()] for label in decoded_labels]\n    # hypotheses_bleu = [pred.split() for pred in decoded_preds]\n    # smoothie = SmoothingFunction().method4\n    # bleu = corpus_bleu(references_bleu, hypotheses_bleu, smoothing_function=smoothie)\n\n    # حساب BERTScore\n    P, R, F1 = bert_score(decoded_preds, decoded_labels, lang=\"ar\")\n    bert_f1 = F1.mean().item()\n\n    return {\n        # \"bleu\": bleu,\n        \"bertscore_f1\": bert_f1\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:58:17.648060Z","iopub.execute_input":"2025-05-12T19:58:17.648692Z","iopub.status.idle":"2025-05-12T19:58:17.653762Z","shell.execute_reply.started":"2025-05-12T19:58:17.648650Z","shell.execute_reply":"2025-05-12T19:58:17.653016Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=500,\n    save_total_limit=1,\n    predict_with_generate=True,\n     report_to=\"none\",\n\n)\n\nclass MetricsCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        small_test_df = test_df.sample(n=100, random_state=42)\n        english_texts = small_test_df[\"english\"].tolist()\n        references = small_test_df[\"arabic\"].tolist()\n\n        preds = generate_translation(english_texts)\n\n     #    # BLEU\n     #  bleu_refs = [[ref.split()] for ref in test_df[\"arabic\"].tolist()]\n     # bleu_preds = [pred.split() for pred in all_preds]\n     # smoothie = SmoothingFunction().method4\n     # bleu = corpus_bleu(bleu_refs, bleu_preds, smoothing_function=smoothie)\n        # BERTScore\n        P, R, F1 = bert_score(preds, references, lang=\"ar\")\n        avg_f1 = F1.mean().item()\n\n        print(f\"\\n✅ Epoch {int(state.epoch)} | BERTScore (F1): {avg_f1:.4f}\")\n        # print(f\"\\n✅ Epoch {int(state.epoch)} - BLEU: {bleu:.4f} | BERTScore (F1): {avg_f1:.4f}\")\n\n# Define trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    callbacks=[MetricsCallback()]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:58:21.639869Z","iopub.execute_input":"2025-05-12T19:58:21.640476Z","iopub.status.idle":"2025-05-12T19:58:21.982100Z","shell.execute_reply.started":"2025-05-12T19:58:21.640446Z","shell.execute_reply":"2025-05-12T19:58:21.981353Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/765604626.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:58:26.292827Z","iopub.execute_input":"2025-05-12T19:58:26.293554Z","iopub.status.idle":"2025-05-12T19:58:26.297388Z","shell.execute_reply.started":"2025-05-12T19:58:26.293525Z","shell.execute_reply":"2025-05-12T19:58:26.296616Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T19:58:30.135860Z","iopub.execute_input":"2025-05-12T19:58:30.136134Z","iopub.status.idle":"2025-05-12T20:41:36.660158Z","shell.execute_reply.started":"2025-05-12T19:58:30.136114Z","shell.execute_reply":"2025-05-12T20:41:36.659416Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5280' max='5280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5280/5280 43:03, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.756300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.538000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.509200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.496500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.488500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.437500</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.427100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.428600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.399100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.416700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.412000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.347100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.367100</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.365700</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.372900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.354800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.320300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.319900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.331300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.326200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.324000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.286600</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.293600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.295900</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.303300</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.274200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.276700</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.274700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.268500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.267500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.258600</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.242300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.249600</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.253400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.261300</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.258200</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.239300</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.231200</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.244300</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.231800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.220400</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.231700</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.226100</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.226300</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.220600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.226400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.222900</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.222100</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.214300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5280, training_loss=0.3162766218185425, metrics={'train_runtime': 2586.0641, 'train_samples_per_second': 65.254, 'train_steps_per_second': 2.042, 'total_flos': 5720349081600000.0, 'train_loss': 0.3162766218185425, 'epoch': 10.0})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# حفظ النموذج\nmodel.save_pretrained(\"saved_model/\")\n\n# حفظ المحول\ntokenizer.save_pretrained(\"saved_model/\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:51:16.312941Z","iopub.execute_input":"2025-05-12T20:51:16.313267Z","iopub.status.idle":"2025-05-12T20:51:17.388973Z","shell.execute_reply.started":"2025-05-12T20:51:16.313245Z","shell.execute_reply":"2025-05-12T20:51:17.388378Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('saved_model/tokenizer_config.json',\n 'saved_model/special_tokens_map.json',\n 'saved_model/vocab.json',\n 'saved_model/source.spm',\n 'saved_model/target.spm',\n 'saved_model/added_tokens.json')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import os\n\nfor file in os.listdir(\"saved_model\"):\n    print(file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:51:32.735079Z","iopub.execute_input":"2025-05-12T20:51:32.735779Z","iopub.status.idle":"2025-05-12T20:51:32.739981Z","shell.execute_reply.started":"2025-05-12T20:51:32.735754Z","shell.execute_reply":"2025-05-12T20:51:32.739277Z"}},"outputs":[{"name":"stdout","text":"config.json\nsource.spm\ngeneration_config.json\nspecial_tokens_map.json\nvocab.json\ntokenizer_config.json\ntarget.spm\nmodel.safetensors\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"\n# Generation configuration\ngeneration_config = GenerationConfig(\n    max_length=128,\n    num_beams=8,\n    repetition_penalty=1.2,\n    length_penalty=1.0,\n    early_stopping=True,\n    forced_eos_token_id=tokenizer.eos_token_id,\n     bad_words_ids=[[62801]] \n)\n\n\ndef generate_translation(texts, batch_size=4):\n    model.eval()\n    translations = []\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        input_ids = inputs.input_ids.to(model.device)\n        attention_mask = inputs.attention_mask.to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                generation_config=generation_config  # استخدم الإعدادات هنا\n            )\n\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        translations.extend(decoded)\n\n    return translations\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:52:39.914831Z","iopub.execute_input":"2025-05-12T20:52:39.915131Z","iopub.status.idle":"2025-05-12T20:52:39.921451Z","shell.execute_reply.started":"2025-05-12T20:52:39.915112Z","shell.execute_reply":"2025-05-12T20:52:39.920733Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\n# ✅ ترجمة مجموعة البيانات كاملة\nenglish_texts = test_df[\"english\"].tolist()\nall_preds = generate_translation(english_texts)\n\n# التقييم الكامل\npreds = all_preds\nreferences = test_df[\"arabic\"].tolist()\nhypotheses = preds\n\nP, R, F1 = bert_score(hypotheses, references, lang=\"ar\")\navg_f1 = F1.mean().item()\n\n\n\n# bleu_score_value = corpus_bleu([[ref.split()] for ref in references], [pred.split() for pred in hypotheses])\n\n# print(f\"\\nBLEU Score on Test Set: {bleu_score_value:.4f}\")\nprint(f\"BERTScore (F1) on Test Set: {avg_f1:.4f}\")\n\nfor i in range(2):\n    print(f\"\\n🔹 English: {test_df['english'].iloc[i]}\")\n    print(f\"🔸 Predicted Arabic: {preds[i]}\")\n    print(f\"✅ Actual Arabic: {test_df['arabic'].iloc[i]}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:53:08.638271Z","iopub.execute_input":"2025-05-12T20:53:08.638571Z","iopub.status.idle":"2025-05-12T21:01:00.237974Z","shell.execute_reply.started":"2025-05-12T20:53:08.638549Z","shell.execute_reply":"2025-05-12T21:01:00.237224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719904c489554e30a5ddbfd1c1ef2333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e881c3b98edb403a83e905d6ef91a63c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77c9a80b3766440298e9397ac6c841ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0a15bcbd174602af66564d418b23f7"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021b314756334464a568ac77ce737b5f"}},"metadata":{}},{"name":"stdout","text":"BERTScore (F1) on Test Set: 0.8564\n\n🔹 English: this theory consists of three parts.\n🔸 Predicted Arabic: تتكون هذه النظرية من ثلاثة أجزاء\n✅ Actual Arabic: تتألف هذه النظرية من ثلاثة أجزاء\n\n🔹 English: everything will change.\n🔸 Predicted Arabic: كل شيء سيتغير\n✅ Actual Arabic: كل شيئ سيتغير\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from transformers import MarianMTModel, MarianTokenizer, GenerationConfig\nimport gradio as gr\n\n# تحميل النموذج المحفوظ\nmodel_path = \"saved_model\"\ntokenizer = MarianTokenizer.from_pretrained(model_path)\nmodel = MarianMTModel.from_pretrained(model_path)\n\n# إعدادات التوليد\ngeneration_config = GenerationConfig(\n    max_length=128,\n    num_beams=4,\n    length_penalty=1.0,\n    early_stopping=True\n)\n\n# دالة الترجمة\ndef generate_translation(texts, batch_size=4):\n    model.eval()\n    translations = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        input_ids = inputs.input_ids.to(model.device)\n        attention_mask = inputs.attention_mask.to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                generation_config=generation_config\n            )\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        translations.extend(decoded)\n    return translations\n\n# دالة تستخدم في الواجهة\ndef translate_and_evaluate(input_text):\n    translated = generate_translation([input_text])[0]\n    return translated\n\n# Gradio Interface\ninterface = gr.Interface(\n    fn=translate_and_evaluate,\n    inputs=gr.Textbox(lines=3, placeholder=\"اكتب الجملة الإنجليزية الموجودة في مجموعة الاختبار...\"),\n    outputs=gr.Textbox(label=\"🔸 Predict Translation\"),\n    title=\"🔤 Translation English ➜ Arabic\",\n    description=\"Enter an English sentence to get its Arabic translation.\"\n)\n\ninterface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T21:01:33.421433Z","iopub.execute_input":"2025-05-12T21:01:33.421727Z","iopub.status.idle":"2025-05-12T21:01:38.368253Z","shell.execute_reply.started":"2025-05-12T21:01:33.421705Z","shell.execute_reply":"2025-05-12T21:01:38.367732Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://f7275fded4ebff8b7c.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f7275fded4ebff8b7c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
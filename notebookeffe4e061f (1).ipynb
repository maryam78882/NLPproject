{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":915247,"sourceType":"datasetVersion","datasetId":492069}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:15.238017Z","iopub.execute_input":"2025-04-30T22:34:15.238288Z","iopub.status.idle":"2025-04-30T22:34:15.244705Z","shell.execute_reply.started":"2025-04-30T22:34:15.238267Z","shell.execute_reply":"2025-04-30T22:34:15.243816Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arabic-to-english-translation-sentences/ara_eng.txt\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets sentencepiece scikit-learn --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:17.662877Z","iopub.execute_input":"2025-04-30T22:34:17.663345Z","iopub.status.idle":"2025-04-30T22:34:20.929538Z","shell.execute_reply.started":"2025-04-30T22:34:17.663315Z","shell.execute_reply":"2025-04-30T22:34:20.928607Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom transformers import (\n    MarianTokenizer, MarianMTModel,\n    Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback, GenerationConfig\n)\nfrom nltk.translate.bleu_score import corpus_bleu,SmoothingFunction\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:23.351550Z","iopub.execute_input":"2025-04-30T22:34:23.351823Z","iopub.status.idle":"2025-04-30T22:34:23.356691Z","shell.execute_reply.started":"2025-04-30T22:34:23.351799Z","shell.execute_reply":"2025-04-30T22:34:23.355947Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"# Load the dataset\npath = \"/kaggle/input/arabic-to-english-translation-sentences/ara_eng.txt\"\ndf = pd.read_csv(path, sep='\\t', names=[\"english\", \"arabic\"], encoding=\"utf-8\")\n\n# Define text cleaning functions\ndef clean_english(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text).strip()\n    return text\n\ndef clean_arabic(text):\n    text = re.sub(r\"[^\\u0600-\\u06FF\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n\n# Apply cleaning\ndf[\"english\"] = df[\"english\"].apply(clean_english)\ndf[\"arabic\"] = df[\"arabic\"].apply(clean_arabic)\nfiltered_df = df[(df[\"english\"].str.split().str.len() >= 3) & (df[\"arabic\"].str.split().str.len() >= 3)]\ndf = df[(df['english'].str.len() > 5) & (df['arabic'].str.len() > 5)]\ndf = df.drop_duplicates(subset=[\"english\"])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:26.497790Z","iopub.execute_input":"2025-04-30T22:34:26.498070Z","iopub.status.idle":"2025-04-30T22:34:27.105764Z","shell.execute_reply.started":"2025-04-30T22:34:26.498050Z","shell.execute_reply":"2025-04-30T22:34:27.105131Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"# Split into training and testing sets\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\ntest_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:30.466726Z","iopub.execute_input":"2025-04-30T22:34:30.467411Z","iopub.status.idle":"2025-04-30T22:34:30.549783Z","shell.execute_reply.started":"2025-04-30T22:34:30.467386Z","shell.execute_reply":"2025-04-30T22:34:30.548999Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"# Load the MarianMT model and tokenizer\nmodel_name = \"Helsinki-NLP/opus-mt-tc-big-en-ar\"\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nmodel = MarianMTModel.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:33.948419Z","iopub.execute_input":"2025-04-30T22:34:33.949203Z","iopub.status.idle":"2025-04-30T22:34:39.249835Z","shell.execute_reply.started":"2025-04-30T22:34:33.949175Z","shell.execute_reply":"2025-04-30T22:34:39.249248Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"}],"execution_count":97},{"cell_type":"code","source":"# Preprocessing function for tokenizing input/target text\nmax_length = 128\n\ndef preprocess(batch):\n    inputs = tokenizer(batch[\"english\"], max_length=max_length, truncation=True, padding=\"max_length\")\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(batch[\"arabic\"], max_length=max_length, truncation=True, padding=\"max_length\")\n    inputs[\"labels\"] = labels[\"input_ids\"]\n    return inputs\n\n# Apply preprocessing\ntrain_dataset = train_dataset.map(preprocess, batched=True, remove_columns=[\"english\", \"arabic\"])\ntest_dataset = test_dataset.map(preprocess, batched=True, remove_columns=[\"english\", \"arabic\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:42.042377Z","iopub.execute_input":"2025-04-30T22:34:42.042878Z","iopub.status.idle":"2025-04-30T22:34:50.493995Z","shell.execute_reply.started":"2025-04-30T22:34:42.042854Z","shell.execute_reply":"2025-04-30T22:34:50.493415Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404ab77c3e3543048f66dd594863c943"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4536 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf5107f27a54edb84331c58e37bad26"}},"metadata":{}}],"execution_count":98},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # حذف -100 المستخدمة في padding\n    labels = [[token for token in label if token != -100] for label in labels]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # تقسيم الكلمات للحساب\n    references = [[label.split()] for label in decoded_labels]\n    hypotheses = [pred.split() for pred in decoded_preds]\n\n    bleu = corpus_bleu(references, hypotheses)\n    return {\"bleu\": bleu}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:34:53.355269Z","iopub.execute_input":"2025-04-30T22:34:53.355827Z","iopub.status.idle":"2025-04-30T22:34:53.360674Z","shell.execute_reply.started":"2025-04-30T22:34:53.355803Z","shell.execute_reply":"2025-04-30T22:34:53.359993Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_steps=1000,\n    save_total_limit=1,\n    predict_with_generate=True\n\n    #  output_dir=\"./results\",\n    # per_device_train_batch_size=16,\n    # per_device_eval_batch_size=16,\n    # num_train_epochs=20,\n    # learning_rate=3e-5,\n    # label_smoothing_factor=0.1,\n    # logging_dir=\"./logs\",\n    # logging_steps=50,\n    # save_steps=500,\n    # save_total_limit=2,\n    # predict_with_generate=True,\n    # evaluation_strategy=\"epoch\",\n    # load_best_model_at_end=True,\n)\n\nclass BLEUCallback(TrainerCallback):\n    def on_evaluate(self, args, state, control, **kwargs):\n        small_test_df = test_df.sample(n=100, random_state=42)\n        english_texts = small_test_df[\"english\"].tolist()\n        references = [[ref.split()] for ref in small_test_df[\"arabic\"]]\n        preds = generate_translation(english_texts)\n        hypotheses = [pred.split() for pred in preds]\n        smoothie = SmoothingFunction().method4\n        bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n        print(f\"\\n✅ BLEU Score after Epoch {int(state.epoch)}: {bleu:.4f}\")\n\n# Define trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics  # ✅ أضف هذا السطر\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:35:02.825780Z","iopub.execute_input":"2025-04-30T22:35:02.826050Z","iopub.status.idle":"2025-04-30T22:35:03.119049Z","shell.execute_reply.started":"2025-04-30T22:35:02.826030Z","shell.execute_reply":"2025-04-30T22:35:03.118494Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_31/3089399776.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":100},{"cell_type":"code","source":"# Start training\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T22:35:09.202115Z","iopub.execute_input":"2025-04-30T22:35:09.202378Z","iopub.status.idle":"2025-05-01T00:31:04.594171Z","shell.execute_reply.started":"2025-04-30T22:35:09.202359Z","shell.execute_reply":"2025-05-01T00:31:04.593585Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5670' max='5670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5670/5670 1:55:53, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>6.581300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.857500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.615200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.845300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.239600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.854600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.620800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.525100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.487900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.459300</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.462000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.403500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.395300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.395000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.477000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.412300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.358500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.388400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.432400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.391600</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.417400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.366700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.398800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.420900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.400700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.353000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.373100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.419100</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.321200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.388200</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.382400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.421000</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.437000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.321200</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.325300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.393400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.387300</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.360600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.429800</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.406500</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.373700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.403100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.341300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.419800</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.444600</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.386900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.372600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.364100</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.363400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.317900</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.391100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.360600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.363800</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.364500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.322000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.290200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.286300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.307400</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.262900</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.274700</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.262300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.258500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.272000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.289400</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.309100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.288300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.282200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.313200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.282700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.315900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.252300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.323500</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.281800</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.319300</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.259200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.309100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.285100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.273900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.248300</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.275400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.286900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.274100</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.303400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.264100</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.271100</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.290200</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.248300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.294400</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.243200</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.318000</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.300500</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.265900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.237000</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.310400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.276000</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.301800</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.292800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.303200</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.356100</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.296200</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.246800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.297100</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.284300</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.290700</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.288900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.285900</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.268700</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.254200</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.309100</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.244600</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.207400</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.202300</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.245600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.250900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.210500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.196700</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.213200</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.217600</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.203600</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.209800</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.267300</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.236700</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.206300</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.248600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.201800</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.228500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.204000</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.246200</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.222100</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.208200</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.192400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.265600</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.202900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.256500</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.231400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.221000</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.204100</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.222900</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.204300</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.222800</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.220200</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.221200</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.190900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.218400</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.209600</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.212200</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.256700</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.220700</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.254800</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.229600</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.237600</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.210800</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.248500</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.226900</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.193800</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.196900</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.232800</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.246000</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.239700</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.208300</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.216200</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.232800</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.218400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.265200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.153500</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.184500</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.171200</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.175300</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.177400</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.171600</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.152100</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.148700</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.187400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.174300</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.152000</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.197200</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.157300</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.147300</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.176900</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.193900</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.172100</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.166600</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.171600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.177300</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.173000</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.170000</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.175700</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.149600</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.171400</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.177100</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.198200</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.169500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.210600</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.199700</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.195600</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.175800</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.188300</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.195700</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.197400</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.188400</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.156100</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.181900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.187800</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.166300</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.176900</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.202500</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.182800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.188600</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.172000</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.169600</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.190300</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.224500</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.187100</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.181600</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.199900</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.201400</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.179000</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.181900</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.168000</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.133500</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.125100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.129400</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.144300</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.140700</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.153300</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.137000</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.121200</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.135300</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.146900</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.149700</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.148700</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.127200</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.121300</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.158100</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.143000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.169800</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.155400</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.149000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.152800</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.147200</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.155400</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.116800</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.123700</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.143600</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.149000</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.132600</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.123500</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.130000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.151500</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.152800</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.145800</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.145900</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.165800</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.124900</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.141100</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.151100</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.158100</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.152400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.145700</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.142200</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.149300</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.143700</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.143800</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.146600</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.158100</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.148900</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.157400</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.150600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.149400</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.160700</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.156600</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.156400</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.118600</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.108900</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.118700</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.101100</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.130300</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.128000</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.115900</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.115100</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.134500</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.114200</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.123700</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.122800</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.126600</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.116800</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.099200</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.122100</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.114300</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.117200</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.111600</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.100800</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.108900</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.131700</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.096600</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.126800</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.121400</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.106300</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.116600</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.125200</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.136800</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.113700</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.111100</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.117200</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.116700</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.127100</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.116400</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.100700</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.112800</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.152900</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.128100</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.118400</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.118900</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.121500</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.111100</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.114200</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.122400</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.130600</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.128300</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.116700</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.104100</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.120100</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.116100</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.112900</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.095800</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.094000</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.099300</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.103200</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.086300</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.103400</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.099200</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.097900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.094400</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.105900</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.093000</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.102200</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.098100</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.081300</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.109100</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.095000</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.091500</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.117000</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.091000</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.099900</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.085600</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.102800</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.092200</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.092000</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.085800</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.095800</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.092900</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.112100</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.102900</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.097400</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.106100</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.097900</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.117400</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.090800</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.092400</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.105500</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.094600</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.095000</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.105300</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.100700</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.110200</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.092600</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.094500</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.098700</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.101900</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.098800</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.090500</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.071900</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.082000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.072800</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.087800</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.083300</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.079700</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.090400</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.079900</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.076400</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.094900</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.087400</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.075200</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.087900</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.085200</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.074400</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.090800</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.077300</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.084800</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.062900</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.083500</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.078300</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>0.084400</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>0.088500</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>0.092300</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.072700</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>0.080900</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>0.079000</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>0.091100</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.090500</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>0.082600</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>0.072000</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>0.071300</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>0.073600</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>0.088600</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>0.080000</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>0.091600</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>0.092800</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>0.084900</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>0.081600</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>0.089700</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>0.086200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>0.084600</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>0.074500</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>0.076200</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.074500</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>0.086200</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>0.069300</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>0.073200</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.075900</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>0.065500</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>0.066000</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>0.070500</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.084800</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>0.061300</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>0.067200</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>0.069000</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>0.074100</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.072700</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>0.075700</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>0.065500</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>0.078900</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.065600</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>0.064700</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>0.070400</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>0.075600</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>0.061400</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.066000</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>0.078900</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>0.090600</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>0.072500</td>\n    </tr>\n    <tr>\n      <td>4840</td>\n      <td>0.069900</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.073500</td>\n    </tr>\n    <tr>\n      <td>4860</td>\n      <td>0.067800</td>\n    </tr>\n    <tr>\n      <td>4870</td>\n      <td>0.078000</td>\n    </tr>\n    <tr>\n      <td>4880</td>\n      <td>0.089000</td>\n    </tr>\n    <tr>\n      <td>4890</td>\n      <td>0.075400</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.073000</td>\n    </tr>\n    <tr>\n      <td>4910</td>\n      <td>0.072100</td>\n    </tr>\n    <tr>\n      <td>4920</td>\n      <td>0.070200</td>\n    </tr>\n    <tr>\n      <td>4930</td>\n      <td>0.073100</td>\n    </tr>\n    <tr>\n      <td>4940</td>\n      <td>0.080500</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.070900</td>\n    </tr>\n    <tr>\n      <td>4960</td>\n      <td>0.077400</td>\n    </tr>\n    <tr>\n      <td>4970</td>\n      <td>0.068600</td>\n    </tr>\n    <tr>\n      <td>4980</td>\n      <td>0.075600</td>\n    </tr>\n    <tr>\n      <td>4990</td>\n      <td>0.079600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.072000</td>\n    </tr>\n    <tr>\n      <td>5010</td>\n      <td>0.068700</td>\n    </tr>\n    <tr>\n      <td>5020</td>\n      <td>0.082200</td>\n    </tr>\n    <tr>\n      <td>5030</td>\n      <td>0.074000</td>\n    </tr>\n    <tr>\n      <td>5040</td>\n      <td>0.076000</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.069500</td>\n    </tr>\n    <tr>\n      <td>5060</td>\n      <td>0.068900</td>\n    </tr>\n    <tr>\n      <td>5070</td>\n      <td>0.065100</td>\n    </tr>\n    <tr>\n      <td>5080</td>\n      <td>0.079200</td>\n    </tr>\n    <tr>\n      <td>5090</td>\n      <td>0.074500</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.067300</td>\n    </tr>\n    <tr>\n      <td>5110</td>\n      <td>0.058300</td>\n    </tr>\n    <tr>\n      <td>5120</td>\n      <td>0.066900</td>\n    </tr>\n    <tr>\n      <td>5130</td>\n      <td>0.062100</td>\n    </tr>\n    <tr>\n      <td>5140</td>\n      <td>0.070000</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.049000</td>\n    </tr>\n    <tr>\n      <td>5160</td>\n      <td>0.054300</td>\n    </tr>\n    <tr>\n      <td>5170</td>\n      <td>0.071400</td>\n    </tr>\n    <tr>\n      <td>5180</td>\n      <td>0.072000</td>\n    </tr>\n    <tr>\n      <td>5190</td>\n      <td>0.069200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.047500</td>\n    </tr>\n    <tr>\n      <td>5210</td>\n      <td>0.070600</td>\n    </tr>\n    <tr>\n      <td>5220</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>5230</td>\n      <td>0.062000</td>\n    </tr>\n    <tr>\n      <td>5240</td>\n      <td>0.065900</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.066600</td>\n    </tr>\n    <tr>\n      <td>5260</td>\n      <td>0.062300</td>\n    </tr>\n    <tr>\n      <td>5270</td>\n      <td>0.065700</td>\n    </tr>\n    <tr>\n      <td>5280</td>\n      <td>0.073800</td>\n    </tr>\n    <tr>\n      <td>5290</td>\n      <td>0.078400</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.065400</td>\n    </tr>\n    <tr>\n      <td>5310</td>\n      <td>0.060100</td>\n    </tr>\n    <tr>\n      <td>5320</td>\n      <td>0.064900</td>\n    </tr>\n    <tr>\n      <td>5330</td>\n      <td>0.056100</td>\n    </tr>\n    <tr>\n      <td>5340</td>\n      <td>0.062200</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.063600</td>\n    </tr>\n    <tr>\n      <td>5360</td>\n      <td>0.068400</td>\n    </tr>\n    <tr>\n      <td>5370</td>\n      <td>0.069100</td>\n    </tr>\n    <tr>\n      <td>5380</td>\n      <td>0.071000</td>\n    </tr>\n    <tr>\n      <td>5390</td>\n      <td>0.074000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.075200</td>\n    </tr>\n    <tr>\n      <td>5410</td>\n      <td>0.059800</td>\n    </tr>\n    <tr>\n      <td>5420</td>\n      <td>0.061900</td>\n    </tr>\n    <tr>\n      <td>5430</td>\n      <td>0.079200</td>\n    </tr>\n    <tr>\n      <td>5440</td>\n      <td>0.062400</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.073600</td>\n    </tr>\n    <tr>\n      <td>5460</td>\n      <td>0.056300</td>\n    </tr>\n    <tr>\n      <td>5470</td>\n      <td>0.058100</td>\n    </tr>\n    <tr>\n      <td>5480</td>\n      <td>0.074200</td>\n    </tr>\n    <tr>\n      <td>5490</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.069600</td>\n    </tr>\n    <tr>\n      <td>5510</td>\n      <td>0.077900</td>\n    </tr>\n    <tr>\n      <td>5520</td>\n      <td>0.071900</td>\n    </tr>\n    <tr>\n      <td>5530</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>5540</td>\n      <td>0.062700</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.070700</td>\n    </tr>\n    <tr>\n      <td>5560</td>\n      <td>0.072400</td>\n    </tr>\n    <tr>\n      <td>5570</td>\n      <td>0.076800</td>\n    </tr>\n    <tr>\n      <td>5580</td>\n      <td>0.056900</td>\n    </tr>\n    <tr>\n      <td>5590</td>\n      <td>0.061400</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.071700</td>\n    </tr>\n    <tr>\n      <td>5610</td>\n      <td>0.068700</td>\n    </tr>\n    <tr>\n      <td>5620</td>\n      <td>0.075500</td>\n    </tr>\n    <tr>\n      <td>5630</td>\n      <td>0.063400</td>\n    </tr>\n    <tr>\n      <td>5640</td>\n      <td>0.067400</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.073600</td>\n    </tr>\n    <tr>\n      <td>5660</td>\n      <td>0.066000</td>\n    </tr>\n    <tr>\n      <td>5670</td>\n      <td>0.058700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61246]], 'forced_eos_token_id': 25897}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5670, training_loss=0.19231362472661168, metrics={'train_runtime': 6954.769, 'train_samples_per_second': 26.089, 'train_steps_per_second': 0.815, 'total_flos': 2.457468080750592e+16, 'train_loss': 0.19231362472661168, 'epoch': 10.0})"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Generation configuration\ngeneration_config = GenerationConfig(\n    max_length=128,\n    num_beams=6,\n    repetition_penalty=1.2,\n    length_penalty=1.0,\n    early_stopping=True,\n    forced_eos_token_id=tokenizer.eos_token_id\n)\n\n\n# Function to translate a list of English sentences\ndef generate_translation(texts, batch_size=2):\n    model.eval()\n    translations = []\n\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i+batch_size]\n\n        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n        input_ids = inputs.input_ids.to(model.device)\n        attention_mask = inputs.attention_mask.to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        translations.extend(decoded)\n\n    return translations\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T00:43:40.808195Z","iopub.execute_input":"2025-05-01T00:43:40.808471Z","iopub.status.idle":"2025-05-01T00:43:40.814771Z","shell.execute_reply.started":"2025-05-01T00:43:40.808451Z","shell.execute_reply":"2025-05-01T00:43:40.813987Z"}},"outputs":[],"execution_count":109},{"cell_type":"code","source":"\nimport torch\n\ndevice = torch.device(\"cpu\")  # استخدام الـ CPU بدلاً من الـ GPU\n\n# تحويل النموذج إلى الـ CPU\nmodel.to(device)\n\nall_preds = []\n\nfor i in range(0, len(english_texts), 10):\n    chunk = english_texts[i : i + 10]\n    chunk_preds = generate_translation(chunk, batch_size=1)\n    all_preds.extend(chunk_preds)\n    \n    # مسح الذاكرة غير المستخدمة\n    torch.cuda.empty_cache()\n\npreds = all_preds\n\n\n# Prepare references and hypotheses\nreferences = [[ref.split()] for ref in small_test_df[\"arabic\"]]\nhypotheses = [pred.split() for pred in preds]\n\n# Compute BLEU score\nbleu_score = corpus_bleu(references, hypotheses)\nprint(f\"\\nBLEU Score on Test Set: {bleu_score:.4f}\")\n\n# Show sample predictions\nfor i in range(5):\n    print(f\"\\n🔹 English: {small_test_df['english'].iloc[i]}\")\n    print(f\"🔸 Predicted Arabic: {preds[i]}\")\n    print(f\"✅ Actual Arabic: {small_test_df['arabic'].iloc[i]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T00:55:53.835594Z","iopub.execute_input":"2025-05-01T00:55:53.835869Z","iopub.status.idle":"2025-05-01T00:58:28.999563Z","shell.execute_reply.started":"2025-05-01T00:55:53.835852Z","shell.execute_reply":"2025-05-01T00:58:28.998819Z"}},"outputs":[{"name":"stdout","text":"\nBLEU Score on Test Set: 0.1983\n\n🔹 English: i didn t like it\n🔸 Predicted Arabic: لم يعجبنى ذلك\n✅ Actual Arabic: لم أحبه\n\n🔹 English: demonstrations of support to the sidibouzid movement took place in paris munich and beirut\n🔸 Predicted Arabic: بدات مظاهرات تضامن مع حركة سيدي بوزيد في باريس ميونخ وبيروت\n✅ Actual Arabic: وما زالت مظاهرات تاييد الحركة مستمرة في باريس موينخ وبيروت\n\n🔹 English: nobody lives in this house\n🔸 Predicted Arabic: لا أحد يسكن في هذا المنزل\n✅ Actual Arabic: لا يعيش أحد في هذا المنزل\n\n🔹 English: venezuela troubles to access blogger com global voices advox\n🔸 Predicted Arabic: فنزويلا مشاكل في الوصول الى موقع الاصوات العالمية\n✅ Actual Arabic: فنزويلا مشاكل تصفح موقع بلوجر الاصوات العالمية\n\n🔹 English: salmanonline posts an article ar about drugs addiction among the lebanese youth he discusses the role played by some political parties and other specialized organizations to combat its widespread\n🔸 Predicted Arabic: ينشر سلمان اونلاين مقال عن ادمان المخدرات بين الشباب اللبناني ويناقش الدور الذي تلعبه بعض الاحزاب السياسية والمنظمات المتخصصة الاخرى لمكافحته واسع الانتشار\n✅ Actual Arabic: ينشر سلمان مقالة على مدونته عن ادمان المخدرات بين الشباب اللبناني ويناقش الدور الذي تلعبه بعض الاحزاب السياسية والموسسات المتخصصة الاخرى لمحاربة انتشار المخدرات\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}